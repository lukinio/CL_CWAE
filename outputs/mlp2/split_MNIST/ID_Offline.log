split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.232 Loss 0.1006
* VALID - Accuracy 98.370 Loss 0.0498
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.415 Loss 0.0451
* VALID - Accuracy 98.260 Loss 0.0501
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.767 Loss 0.0350
* VALID - Accuracy 98.820 Loss 0.0389
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.095 Loss 0.0261
* VALID - Accuracy 98.680 Loss 0.0433
* VALID - Accuracy 98.680 Loss 0.0433
OrderedDict([('All', {'All': 98.68})])
Task All average acc: 98.68
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 9.868 std: 29.604000000000003
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.293 Loss 0.0998
* VALID - Accuracy 98.360 Loss 0.0482
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.353 Loss 0.0480
* VALID - Accuracy 98.410 Loss 0.0491
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.780 Loss 0.0349
* VALID - Accuracy 98.730 Loss 0.0413
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.120 Loss 0.0257
* VALID - Accuracy 98.890 Loss 0.0378
* VALID - Accuracy 98.890 Loss 0.0378
OrderedDict([('All', {'All': 98.89})])
Task All average acc: 98.89
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89  0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 19.756999999999998 std: 39.514027901493414
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.335 Loss 0.0983
* VALID - Accuracy 97.950 Loss 0.0575
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.408 Loss 0.0487
* VALID - Accuracy 98.360 Loss 0.0478
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.808 Loss 0.0344
* VALID - Accuracy 98.340 Loss 0.0498
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.093 Loss 0.0261
* VALID - Accuracy 98.700 Loss 0.0415
* VALID - Accuracy 98.700 Loss 0.0415
OrderedDict([('All', {'All': 98.7})])
Task All average acc: 98.7
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7   0.    0.    0.    0.    0.    0.    0.  ]
mean: 29.627 std: 45.256019721137655
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.157 Loss 0.1023
* VALID - Accuracy 97.800 Loss 0.0583
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.337 Loss 0.0468
* VALID - Accuracy 98.290 Loss 0.0470
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.835 Loss 0.0340
* VALID - Accuracy 98.650 Loss 0.0409
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.092 Loss 0.0264
* VALID - Accuracy 98.600 Loss 0.0425
* VALID - Accuracy 98.600 Loss 0.0425
OrderedDict([('All', {'All': 98.6})])
Task All average acc: 98.6
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6   0.    0.    0.    0.    0.    0.  ]
mean: 39.487 std: 48.3615475455449
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.132 Loss 0.1038
* VALID - Accuracy 98.000 Loss 0.0565
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.365 Loss 0.0473
* VALID - Accuracy 98.540 Loss 0.0459
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.797 Loss 0.0352
* VALID - Accuracy 98.780 Loss 0.0400
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.068 Loss 0.0265
* VALID - Accuracy 98.650 Loss 0.0413
* VALID - Accuracy 98.650 Loss 0.0413
OrderedDict([('All', {'All': 98.65})])
Task All average acc: 98.65
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65  0.    0.    0.    0.    0.  ]
mean: 49.352 std: 49.35204956230288
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.332 Loss 0.0967
* VALID - Accuracy 98.110 Loss 0.0572
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.342 Loss 0.0477
* VALID - Accuracy 98.710 Loss 0.0397
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.842 Loss 0.0343
* VALID - Accuracy 98.800 Loss 0.0379
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.090 Loss 0.0254
* VALID - Accuracy 98.480 Loss 0.0442
* VALID - Accuracy 98.480 Loss 0.0442
OrderedDict([('All', {'All': 98.48})])
Task All average acc: 98.48
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65 98.48  0.    0.    0.    0.  ]
mean: 59.2 std: 48.33669144656055
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.297 Loss 0.0989
* VALID - Accuracy 97.780 Loss 0.0657
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.403 Loss 0.0457
* VALID - Accuracy 98.670 Loss 0.0362
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.863 Loss 0.0325
* VALID - Accuracy 98.730 Loss 0.0376
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.152 Loss 0.0250
* VALID - Accuracy 98.840 Loss 0.0374
* VALID - Accuracy 98.840 Loss 0.0374
OrderedDict([('All', {'All': 98.84})])
Task All average acc: 98.84
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65 98.48 98.84  0.    0.    0.  ]
mean: 69.084 std: 45.22622296853895
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.205 Loss 0.1015
* VALID - Accuracy 97.990 Loss 0.0544
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.370 Loss 0.0481
* VALID - Accuracy 98.690 Loss 0.0393
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.737 Loss 0.0355
* VALID - Accuracy 98.710 Loss 0.0402
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.097 Loss 0.0266
* VALID - Accuracy 98.530 Loss 0.0440
* VALID - Accuracy 98.530 Loss 0.0440
OrderedDict([('All', {'All': 98.53})])
Task All average acc: 98.53
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65 98.48 98.84 98.53  0.    0.  ]
mean: 78.937 std: 39.468676453613185
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.220 Loss 0.1002
* VALID - Accuracy 98.070 Loss 0.0570
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.315 Loss 0.0481
* VALID - Accuracy 98.500 Loss 0.0427
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.857 Loss 0.0329
* VALID - Accuracy 98.540 Loss 0.0460
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.078 Loss 0.0271
* VALID - Accuracy 98.760 Loss 0.0391
* VALID - Accuracy 98.760 Loss 0.0391
OrderedDict([('All', {'All': 98.76})])
Task All average acc: 98.76
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65 98.48 98.84 98.53 98.76  0.  ]
mean: 88.813 std: 29.604580405741274
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
MLP2(
  (linear): Sequential(
    (0): Linear(in_features=784, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=256, bias=True)
    (5): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 1460482
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 96.240 Loss 0.0997
* VALID - Accuracy 98.250 Loss 0.0553
Epoch:1
LR: 0.001
* TRAIN - Accuracy 98.393 Loss 0.0464
* VALID - Accuracy 98.480 Loss 0.0435
Epoch:2
LR: 0.001
* TRAIN - Accuracy 98.767 Loss 0.0352
* VALID - Accuracy 98.860 Loss 0.0363
Epoch:3
LR: 0.001
* TRAIN - Accuracy 99.155 Loss 0.0252
* VALID - Accuracy 98.860 Loss 0.0407
* VALID - Accuracy 98.860 Loss 0.0407
OrderedDict([('All', {'All': 98.86})])
Task All average acc: 98.86
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [98.68 98.89 98.7  98.6  98.65 98.48 98.84 98.53 98.76 98.86]
mean: 98.699 std: 0.13232157798333566
reg_coef: 0.0 mean: 98.699 std: 0.13232157798333566
