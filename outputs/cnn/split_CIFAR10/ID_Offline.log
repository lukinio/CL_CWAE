Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 75.422 Loss 0.5039
* VALID - Accuracy 78.380 Loss 0.4544
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.454 Loss 0.3865
* VALID - Accuracy 84.140 Loss 0.3609
Epoch:2
LR: 0.001
* TRAIN - Accuracy 85.016 Loss 0.3371
* VALID - Accuracy 86.080 Loss 0.3193
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.510 Loss 0.3058
* VALID - Accuracy 87.330 Loss 0.2954
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.028 Loss 0.2781
* VALID - Accuracy 87.820 Loss 0.2877
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.072 Loss 0.2580
* VALID - Accuracy 83.830 Loss 0.3902
Epoch:6
LR: 0.001
* TRAIN - Accuracy 89.818 Loss 0.2434
* VALID - Accuracy 83.110 Loss 0.4043
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.582 Loss 0.2287
* VALID - Accuracy 87.020 Loss 0.2983
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.358 Loss 0.2110
* VALID - Accuracy 91.060 Loss 0.2250
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.730 Loss 0.2012
* VALID - Accuracy 91.580 Loss 0.2189
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.234 Loss 0.1887
* VALID - Accuracy 90.180 Loss 0.2368
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.650 Loss 0.1827
* VALID - Accuracy 91.320 Loss 0.2126
* VALID - Accuracy 91.320 Loss 0.2126
OrderedDict([('All', {'All': 91.32})])
Task All average acc: 91.32
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 9.132 std: 27.395999999999997
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.356 Loss 0.5171
* VALID - Accuracy 78.170 Loss 0.4460
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.432 Loss 0.3862
* VALID - Accuracy 72.570 Loss 0.6219
Epoch:2
LR: 0.001
* TRAIN - Accuracy 84.822 Loss 0.3376
* VALID - Accuracy 84.550 Loss 0.3502
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.698 Loss 0.3036
* VALID - Accuracy 84.240 Loss 0.3561
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.166 Loss 0.2781
* VALID - Accuracy 88.160 Loss 0.2722
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.198 Loss 0.2573
* VALID - Accuracy 89.040 Loss 0.2627
Epoch:6
LR: 0.001
* TRAIN - Accuracy 89.980 Loss 0.2385
* VALID - Accuracy 87.720 Loss 0.3011
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.662 Loss 0.2259
* VALID - Accuracy 90.590 Loss 0.2394
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.248 Loss 0.2142
* VALID - Accuracy 90.790 Loss 0.2222
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.736 Loss 0.2005
* VALID - Accuracy 91.520 Loss 0.2064
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.284 Loss 0.1876
* VALID - Accuracy 91.230 Loss 0.2157
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.804 Loss 0.1780
* VALID - Accuracy 91.220 Loss 0.2254
* VALID - Accuracy 91.220 Loss 0.2254
OrderedDict([('All', {'All': 91.22})])
Task All average acc: 91.22
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22  0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 18.253999999999998 std: 36.50800684781354
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.966 Loss 0.5120
* VALID - Accuracy 78.370 Loss 0.4633
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.424 Loss 0.3845
* VALID - Accuracy 80.200 Loss 0.4143
Epoch:2
LR: 0.001
* TRAIN - Accuracy 85.026 Loss 0.3378
* VALID - Accuracy 85.180 Loss 0.3320
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.362 Loss 0.3082
* VALID - Accuracy 86.280 Loss 0.3192
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.110 Loss 0.2781
* VALID - Accuracy 88.400 Loss 0.2762
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.120 Loss 0.2568
* VALID - Accuracy 89.110 Loss 0.2676
Epoch:6
LR: 0.001
* TRAIN - Accuracy 89.888 Loss 0.2405
* VALID - Accuracy 88.010 Loss 0.3089
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.632 Loss 0.2265
* VALID - Accuracy 90.480 Loss 0.2386
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.224 Loss 0.2131
* VALID - Accuracy 88.410 Loss 0.2811
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.762 Loss 0.2004
* VALID - Accuracy 88.470 Loss 0.2768
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.366 Loss 0.1871
* VALID - Accuracy 91.170 Loss 0.2160
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.592 Loss 0.1813
* VALID - Accuracy 92.010 Loss 0.1991
* VALID - Accuracy 92.010 Loss 0.1991
OrderedDict([('All', {'All': 92.01})])
Task All average acc: 92.01
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01  0.    0.    0.    0.    0.    0.    0.  ]
mean: 27.455000000000002 std: 41.938646437385174
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 73.744 Loss 0.5270
* VALID - Accuracy 76.500 Loss 0.5070
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.126 Loss 0.3910
* VALID - Accuracy 82.350 Loss 0.3863
Epoch:2
LR: 0.001
* TRAIN - Accuracy 84.748 Loss 0.3404
* VALID - Accuracy 76.580 Loss 0.4860
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.836 Loss 0.3025
* VALID - Accuracy 87.920 Loss 0.2832
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.132 Loss 0.2777
* VALID - Accuracy 84.980 Loss 0.3623
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.196 Loss 0.2566
* VALID - Accuracy 89.030 Loss 0.2554
Epoch:6
LR: 0.001
* TRAIN - Accuracy 90.142 Loss 0.2360
* VALID - Accuracy 90.060 Loss 0.2444
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.982 Loss 0.2189
* VALID - Accuracy 89.030 Loss 0.2588
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.442 Loss 0.2064
* VALID - Accuracy 91.280 Loss 0.2217
Epoch:9
LR: 0.001
* TRAIN - Accuracy 92.180 Loss 0.1933
* VALID - Accuracy 89.820 Loss 0.2564
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.586 Loss 0.1837
* VALID - Accuracy 91.670 Loss 0.1998
Epoch:11
LR: 0.001
* TRAIN - Accuracy 93.084 Loss 0.1730
* VALID - Accuracy 91.320 Loss 0.2104
* VALID - Accuracy 91.320 Loss 0.2104
OrderedDict([('All', {'All': 91.32})])
Task All average acc: 91.32
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32  0.    0.    0.    0.    0.    0.  ]
mean: 36.587 std: 44.81018590677793
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.972 Loss 0.5107
* VALID - Accuracy 77.930 Loss 0.4643
Epoch:1
LR: 0.001
* TRAIN - Accuracy 81.958 Loss 0.3935
* VALID - Accuracy 83.000 Loss 0.3761
Epoch:2
LR: 0.001
* TRAIN - Accuracy 84.746 Loss 0.3404
* VALID - Accuracy 85.710 Loss 0.3168
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.572 Loss 0.3084
* VALID - Accuracy 87.390 Loss 0.3021
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.078 Loss 0.2798
* VALID - Accuracy 86.660 Loss 0.3101
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.014 Loss 0.2585
* VALID - Accuracy 89.340 Loss 0.2509
Epoch:6
LR: 0.001
* TRAIN - Accuracy 89.720 Loss 0.2433
* VALID - Accuracy 89.680 Loss 0.2510
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.540 Loss 0.2278
* VALID - Accuracy 90.130 Loss 0.2401
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.346 Loss 0.2128
* VALID - Accuracy 90.810 Loss 0.2266
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.806 Loss 0.1986
* VALID - Accuracy 91.090 Loss 0.2238
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.342 Loss 0.1870
* VALID - Accuracy 90.930 Loss 0.2199
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.658 Loss 0.1793
* VALID - Accuracy 91.310 Loss 0.2171
* VALID - Accuracy 91.310 Loss 0.2171
OrderedDict([('All', {'All': 91.31})])
Task All average acc: 91.31
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31  0.    0.    0.    0.    0.  ]
mean: 45.718 std: 45.71845815422913
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 75.552 Loss 0.4987
* VALID - Accuracy 79.420 Loss 0.4412
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.986 Loss 0.3776
* VALID - Accuracy 85.350 Loss 0.3332
Epoch:2
LR: 0.001
* TRAIN - Accuracy 85.514 Loss 0.3295
* VALID - Accuracy 86.020 Loss 0.3096
Epoch:3
LR: 0.001
* TRAIN - Accuracy 87.178 Loss 0.2965
* VALID - Accuracy 87.610 Loss 0.2956
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.468 Loss 0.2702
* VALID - Accuracy 88.210 Loss 0.2708
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.536 Loss 0.2496
* VALID - Accuracy 83.560 Loss 0.3893
Epoch:6
LR: 0.001
* TRAIN - Accuracy 90.382 Loss 0.2289
* VALID - Accuracy 89.540 Loss 0.2566
Epoch:7
LR: 0.001
* TRAIN - Accuracy 91.002 Loss 0.2172
* VALID - Accuracy 91.140 Loss 0.2102
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.656 Loss 0.2030
* VALID - Accuracy 88.710 Loss 0.2731
Epoch:9
LR: 0.001
* TRAIN - Accuracy 92.266 Loss 0.1921
* VALID - Accuracy 90.440 Loss 0.2348
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.578 Loss 0.1823
* VALID - Accuracy 91.950 Loss 0.2009
Epoch:11
LR: 0.001
* TRAIN - Accuracy 93.122 Loss 0.1726
* VALID - Accuracy 91.470 Loss 0.2183
* VALID - Accuracy 91.470 Loss 0.2183
OrderedDict([('All', {'All': 91.47})])
Task All average acc: 91.47
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31 91.47  0.    0.    0.    0.  ]
mean: 54.864999999999995 std: 44.79755356043452
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.858 Loss 0.5118
* VALID - Accuracy 81.920 Loss 0.3993
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.172 Loss 0.3924
* VALID - Accuracy 83.390 Loss 0.3680
Epoch:2
LR: 0.001
* TRAIN - Accuracy 84.998 Loss 0.3387
* VALID - Accuracy 81.500 Loss 0.4032
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.744 Loss 0.3035
* VALID - Accuracy 83.500 Loss 0.3477
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.054 Loss 0.2786
* VALID - Accuracy 86.790 Loss 0.2978
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.146 Loss 0.2553
* VALID - Accuracy 87.600 Loss 0.2999
Epoch:6
LR: 0.001
* TRAIN - Accuracy 90.068 Loss 0.2370
* VALID - Accuracy 90.260 Loss 0.2420
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.740 Loss 0.2241
* VALID - Accuracy 90.150 Loss 0.2344
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.400 Loss 0.2083
* VALID - Accuracy 89.320 Loss 0.2600
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.700 Loss 0.1967
* VALID - Accuracy 91.220 Loss 0.2167
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.598 Loss 0.1818
* VALID - Accuracy 91.260 Loss 0.2098
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.984 Loss 0.1740
* VALID - Accuracy 91.350 Loss 0.2155
* VALID - Accuracy 91.350 Loss 0.2155
OrderedDict([('All', {'All': 91.35})])
Task All average acc: 91.35
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31 91.47 91.35  0.    0.    0.  ]
mean: 64.0 std: 41.89834459737043
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 75.052 Loss 0.5068
* VALID - Accuracy 80.800 Loss 0.4209
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.284 Loss 0.3876
* VALID - Accuracy 84.180 Loss 0.3529
Epoch:2
LR: 0.001
* TRAIN - Accuracy 85.270 Loss 0.3313
* VALID - Accuracy 85.980 Loss 0.3174
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.854 Loss 0.3008
* VALID - Accuracy 88.180 Loss 0.2865
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.072 Loss 0.2765
* VALID - Accuracy 84.650 Loss 0.3838
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.328 Loss 0.2514
* VALID - Accuracy 89.630 Loss 0.2510
Epoch:6
LR: 0.001
* TRAIN - Accuracy 90.224 Loss 0.2339
* VALID - Accuracy 90.160 Loss 0.2381
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.754 Loss 0.2232
* VALID - Accuracy 89.890 Loss 0.2470
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.262 Loss 0.2098
* VALID - Accuracy 90.970 Loss 0.2233
Epoch:9
LR: 0.001
* TRAIN - Accuracy 92.098 Loss 0.1967
* VALID - Accuracy 91.260 Loss 0.2305
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.622 Loss 0.1840
* VALID - Accuracy 91.700 Loss 0.2087
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.900 Loss 0.1762
* VALID - Accuracy 91.760 Loss 0.2031
* VALID - Accuracy 91.760 Loss 0.2031
OrderedDict([('All', {'All': 91.76})])
Task All average acc: 91.76
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31 91.47 91.35 91.76  0.    0.  ]
mean: 73.176 std: 36.58871498153495
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.664 Loss 0.5148
* VALID - Accuracy 80.610 Loss 0.4379
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.630 Loss 0.3816
* VALID - Accuracy 69.190 Loss 0.9045
Epoch:2
LR: 0.001
* TRAIN - Accuracy 85.008 Loss 0.3345
* VALID - Accuracy 79.670 Loss 0.4494
Epoch:3
LR: 0.001
* TRAIN - Accuracy 87.112 Loss 0.2996
* VALID - Accuracy 86.760 Loss 0.3105
Epoch:4
LR: 0.001
* TRAIN - Accuracy 88.344 Loss 0.2735
* VALID - Accuracy 86.620 Loss 0.3215
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.212 Loss 0.2553
* VALID - Accuracy 88.640 Loss 0.2777
Epoch:6
LR: 0.001
* TRAIN - Accuracy 90.160 Loss 0.2356
* VALID - Accuracy 89.750 Loss 0.2545
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.772 Loss 0.2213
* VALID - Accuracy 89.910 Loss 0.2366
Epoch:8
LR: 0.001
* TRAIN - Accuracy 91.558 Loss 0.2083
* VALID - Accuracy 88.500 Loss 0.2953
Epoch:9
LR: 0.001
* TRAIN - Accuracy 92.086 Loss 0.1950
* VALID - Accuracy 89.120 Loss 0.3001
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.518 Loss 0.1835
* VALID - Accuracy 89.870 Loss 0.2456
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.946 Loss 0.1733
* VALID - Accuracy 90.790 Loss 0.2356
* VALID - Accuracy 90.790 Loss 0.2356
OrderedDict([('All', {'All': 90.79})])
Task All average acc: 90.79
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31 91.47 91.35 91.76 90.79  0.  ]
mean: 82.255 std: 27.420036925576888
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=2, bias=True)
  )
)
#parameter of model: 960802
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 74.338 Loss 0.5183
* VALID - Accuracy 79.670 Loss 0.4450
Epoch:1
LR: 0.001
* TRAIN - Accuracy 82.086 Loss 0.3910
* VALID - Accuracy 81.320 Loss 0.4139
Epoch:2
LR: 0.001
* TRAIN - Accuracy 84.650 Loss 0.3393
* VALID - Accuracy 86.990 Loss 0.3021
Epoch:3
LR: 0.001
* TRAIN - Accuracy 86.856 Loss 0.3050
* VALID - Accuracy 83.500 Loss 0.3755
Epoch:4
LR: 0.001
* TRAIN - Accuracy 87.704 Loss 0.2834
* VALID - Accuracy 84.990 Loss 0.3343
Epoch:5
LR: 0.001
* TRAIN - Accuracy 89.040 Loss 0.2596
* VALID - Accuracy 87.170 Loss 0.2926
Epoch:6
LR: 0.001
* TRAIN - Accuracy 89.828 Loss 0.2418
* VALID - Accuracy 89.180 Loss 0.2537
Epoch:7
LR: 0.001
* TRAIN - Accuracy 90.466 Loss 0.2274
* VALID - Accuracy 90.240 Loss 0.2303
Epoch:8
LR: 0.001
* TRAIN - Accuracy 90.904 Loss 0.2176
* VALID - Accuracy 90.240 Loss 0.2444
Epoch:9
LR: 0.001
* TRAIN - Accuracy 91.732 Loss 0.2014
* VALID - Accuracy 91.020 Loss 0.2179
Epoch:10
LR: 0.001
* TRAIN - Accuracy 92.256 Loss 0.1909
* VALID - Accuracy 90.890 Loss 0.2238
Epoch:11
LR: 0.001
* TRAIN - Accuracy 92.738 Loss 0.1787
* VALID - Accuracy 91.030 Loss 0.2277
* VALID - Accuracy 91.030 Loss 0.2277
OrderedDict([('All', {'All': 91.03})])
Task All average acc: 91.03
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [91.32 91.22 92.01 91.32 91.31 91.47 91.35 91.76 90.79 91.03]
mean: 91.35799999999999 std: 0.3246166970443763
reg_coef: 0.0 mean: 91.35799999999999 std: 0.3246166970443763
