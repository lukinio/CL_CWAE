Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 49.226 Loss 1.3746
* VALID - Accuracy 60.990 Loss 1.1576
Epoch:1
LR: 0.001
* TRAIN - Accuracy 68.428 Loss 0.8903
* VALID - Accuracy 69.980 Loss 0.8754
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.288 Loss 0.7252
* VALID - Accuracy 72.660 Loss 0.7945
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.394 Loss 0.6174
* VALID - Accuracy 79.530 Loss 0.5993
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.602 Loss 0.5575
* VALID - Accuracy 80.200 Loss 0.5701
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.726 Loss 0.5009
* VALID - Accuracy 81.800 Loss 0.5337
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.974 Loss 0.4621
* VALID - Accuracy 82.350 Loss 0.5292
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.388 Loss 0.4241
* VALID - Accuracy 79.910 Loss 0.6120
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.308 Loss 0.3963
* VALID - Accuracy 83.550 Loss 0.4965
Epoch:9
LR: 0.001
* TRAIN - Accuracy 87.002 Loss 0.3760
* VALID - Accuracy 85.190 Loss 0.4568
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.836 Loss 0.3515
* VALID - Accuracy 83.340 Loss 0.5229
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.444 Loss 0.3311
* VALID - Accuracy 81.860 Loss 0.5624
* VALID - Accuracy 81.860 Loss 0.5624
OrderedDict([('All', {'All': 81.86})])
Task All average acc: 81.86
===Summary of experiment repeats: 1 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 8.186 std: 24.558000000000003
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 49.022 Loss 1.3881
* VALID - Accuracy 61.840 Loss 1.0969
Epoch:1
LR: 0.001
* TRAIN - Accuracy 68.622 Loss 0.8747
* VALID - Accuracy 70.130 Loss 0.8672
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.654 Loss 0.7182
* VALID - Accuracy 73.640 Loss 0.7656
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.364 Loss 0.6236
* VALID - Accuracy 76.730 Loss 0.6703
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.698 Loss 0.5550
* VALID - Accuracy 76.260 Loss 0.7541
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.560 Loss 0.5034
* VALID - Accuracy 78.800 Loss 0.6055
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.920 Loss 0.4619
* VALID - Accuracy 81.120 Loss 0.5585
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.260 Loss 0.4270
* VALID - Accuracy 79.360 Loss 0.6259
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.046 Loss 0.4046
* VALID - Accuracy 81.500 Loss 0.5480
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.900 Loss 0.3798
* VALID - Accuracy 82.030 Loss 0.5505
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.596 Loss 0.3583
* VALID - Accuracy 85.030 Loss 0.4617
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.502 Loss 0.3321
* VALID - Accuracy 86.660 Loss 0.4082
* VALID - Accuracy 86.660 Loss 0.4082
OrderedDict([('All', {'All': 86.66})])
Task All average acc: 86.66
===Summary of experiment repeats: 2 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66  0.    0.    0.    0.    0.    0.    0.    0.  ]
mean: 16.851999999999997 std: 33.721085629024465
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 52.444 Loss 1.3064
* VALID - Accuracy 56.640 Loss 1.2492
Epoch:1
LR: 0.001
* TRAIN - Accuracy 69.450 Loss 0.8589
* VALID - Accuracy 71.510 Loss 0.8350
Epoch:2
LR: 0.001
* TRAIN - Accuracy 75.268 Loss 0.6991
* VALID - Accuracy 74.890 Loss 0.7361
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.540 Loss 0.6110
* VALID - Accuracy 78.290 Loss 0.6477
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.918 Loss 0.5447
* VALID - Accuracy 73.140 Loss 0.8290
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.830 Loss 0.4926
* VALID - Accuracy 78.620 Loss 0.6587
Epoch:6
LR: 0.001
* TRAIN - Accuracy 84.442 Loss 0.4508
* VALID - Accuracy 82.780 Loss 0.5153
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.132 Loss 0.4282
* VALID - Accuracy 83.100 Loss 0.5088
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.346 Loss 0.3930
* VALID - Accuracy 84.430 Loss 0.4701
Epoch:9
LR: 0.001
* TRAIN - Accuracy 87.180 Loss 0.3653
* VALID - Accuracy 84.670 Loss 0.4663
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.702 Loss 0.3521
* VALID - Accuracy 86.290 Loss 0.4096
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.580 Loss 0.3278
* VALID - Accuracy 86.540 Loss 0.3964
* VALID - Accuracy 86.540 Loss 0.3964
OrderedDict([('All', {'All': 86.54})])
Task All average acc: 86.54
===Summary of experiment repeats: 3 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54  0.    0.    0.    0.    0.    0.    0.  ]
mean: 25.506 std: 38.98028532476385
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 49.244 Loss 1.3841
* VALID - Accuracy 60.230 Loss 1.0916
Epoch:1
LR: 0.001
* TRAIN - Accuracy 68.598 Loss 0.8809
* VALID - Accuracy 73.660 Loss 0.7597
Epoch:2
LR: 0.001
* TRAIN - Accuracy 75.066 Loss 0.7112
* VALID - Accuracy 65.310 Loss 1.1030
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.380 Loss 0.6164
* VALID - Accuracy 79.710 Loss 0.6010
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.696 Loss 0.5529
* VALID - Accuracy 79.220 Loss 0.6066
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.836 Loss 0.4947
* VALID - Accuracy 81.580 Loss 0.5461
Epoch:6
LR: 0.001
* TRAIN - Accuracy 84.036 Loss 0.4592
* VALID - Accuracy 81.550 Loss 0.5432
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.194 Loss 0.4281
* VALID - Accuracy 82.890 Loss 0.5133
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.496 Loss 0.3902
* VALID - Accuracy 83.060 Loss 0.5117
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.926 Loss 0.3743
* VALID - Accuracy 79.230 Loss 0.6311
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.844 Loss 0.3490
* VALID - Accuracy 85.720 Loss 0.4311
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.498 Loss 0.3309
* VALID - Accuracy 86.410 Loss 0.4203
* VALID - Accuracy 86.410 Loss 0.4203
OrderedDict([('All', {'All': 86.41})])
Task All average acc: 86.41
===Summary of experiment repeats: 4 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41  0.    0.    0.    0.    0.    0.  ]
mean: 34.147 std: 41.8410071700001
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 49.546 Loss 1.3770
* VALID - Accuracy 57.300 Loss 1.2793
Epoch:1
LR: 0.001
* TRAIN - Accuracy 67.664 Loss 0.8973
* VALID - Accuracy 65.090 Loss 1.0499
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.362 Loss 0.7253
* VALID - Accuracy 75.750 Loss 0.7219
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.156 Loss 0.6282
* VALID - Accuracy 73.230 Loss 0.8101
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.600 Loss 0.5587
* VALID - Accuracy 77.640 Loss 0.6518
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.212 Loss 0.5097
* VALID - Accuracy 80.800 Loss 0.5747
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.586 Loss 0.4727
* VALID - Accuracy 79.300 Loss 0.6351
Epoch:7
LR: 0.001
* TRAIN - Accuracy 84.652 Loss 0.4344
* VALID - Accuracy 83.530 Loss 0.4872
Epoch:8
LR: 0.001
* TRAIN - Accuracy 85.876 Loss 0.4033
* VALID - Accuracy 83.560 Loss 0.4938
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.382 Loss 0.3857
* VALID - Accuracy 82.700 Loss 0.5270
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.496 Loss 0.3601
* VALID - Accuracy 85.090 Loss 0.4505
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.096 Loss 0.3394
* VALID - Accuracy 84.110 Loss 0.4926
* VALID - Accuracy 84.110 Loss 0.4926
OrderedDict([('All', {'All': 84.11})])
Task All average acc: 84.11
===Summary of experiment repeats: 5 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11  0.    0.    0.    0.    0.  ]
mean: 42.558 std: 42.57878974325128
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 50.314 Loss 1.3446
* VALID - Accuracy 63.100 Loss 0.9983
Epoch:1
LR: 0.001
* TRAIN - Accuracy 68.964 Loss 0.8696
* VALID - Accuracy 71.590 Loss 0.8578
Epoch:2
LR: 0.001
* TRAIN - Accuracy 75.024 Loss 0.7056
* VALID - Accuracy 72.860 Loss 0.8003
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.472 Loss 0.6124
* VALID - Accuracy 78.340 Loss 0.6410
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.674 Loss 0.5479
* VALID - Accuracy 78.770 Loss 0.6215
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.542 Loss 0.4995
* VALID - Accuracy 79.260 Loss 0.6102
Epoch:6
LR: 0.001
* TRAIN - Accuracy 84.128 Loss 0.4571
* VALID - Accuracy 81.860 Loss 0.5375
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.136 Loss 0.4266
* VALID - Accuracy 83.280 Loss 0.4900
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.110 Loss 0.3999
* VALID - Accuracy 85.350 Loss 0.4431
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.994 Loss 0.3729
* VALID - Accuracy 83.320 Loss 0.5181
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.610 Loss 0.3540
* VALID - Accuracy 83.920 Loss 0.5071
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.332 Loss 0.3336
* VALID - Accuracy 85.940 Loss 0.4314
* VALID - Accuracy 85.940 Loss 0.4314
OrderedDict([('All', {'All': 85.94})])
Task All average acc: 85.94
===Summary of experiment repeats: 6 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11 85.94  0.    0.    0.    0.  ]
mean: 51.152 std: 41.787294193331064
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 46.416 Loss 1.4439
* VALID - Accuracy 52.630 Loss 1.3279
Epoch:1
LR: 0.001
* TRAIN - Accuracy 66.740 Loss 0.9278
* VALID - Accuracy 68.510 Loss 0.9077
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.054 Loss 0.7366
* VALID - Accuracy 74.450 Loss 0.7503
Epoch:3
LR: 0.001
* TRAIN - Accuracy 77.800 Loss 0.6340
* VALID - Accuracy 76.540 Loss 0.6978
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.190 Loss 0.5680
* VALID - Accuracy 79.370 Loss 0.6171
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.012 Loss 0.5159
* VALID - Accuracy 81.110 Loss 0.5586
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.432 Loss 0.4743
* VALID - Accuracy 80.020 Loss 0.6038
Epoch:7
LR: 0.001
* TRAIN - Accuracy 84.948 Loss 0.4352
* VALID - Accuracy 76.900 Loss 0.7074
Epoch:8
LR: 0.001
* TRAIN - Accuracy 85.814 Loss 0.4088
* VALID - Accuracy 83.600 Loss 0.4778
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.688 Loss 0.3834
* VALID - Accuracy 82.220 Loss 0.5307
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.394 Loss 0.3652
* VALID - Accuracy 82.020 Loss 0.5458
Epoch:11
LR: 0.001
* TRAIN - Accuracy 87.966 Loss 0.3434
* VALID - Accuracy 82.600 Loss 0.5176
* VALID - Accuracy 82.600 Loss 0.5176
OrderedDict([('All', {'All': 82.6})])
Task All average acc: 82.6
===Summary of experiment repeats: 7 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11 85.94 82.6   0.    0.    0.  ]
mean: 59.412 std: 38.92550983609592
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 48.652 Loss 1.3943
* VALID - Accuracy 56.770 Loss 1.2348
Epoch:1
LR: 0.001
* TRAIN - Accuracy 67.282 Loss 0.9127
* VALID - Accuracy 70.870 Loss 0.8243
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.078 Loss 0.7334
* VALID - Accuracy 75.880 Loss 0.7023
Epoch:3
LR: 0.001
* TRAIN - Accuracy 77.850 Loss 0.6301
* VALID - Accuracy 74.600 Loss 0.7757
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.292 Loss 0.5617
* VALID - Accuracy 78.740 Loss 0.6467
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.218 Loss 0.5105
* VALID - Accuracy 79.490 Loss 0.6066
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.736 Loss 0.4703
* VALID - Accuracy 82.560 Loss 0.5269
Epoch:7
LR: 0.001
* TRAIN - Accuracy 84.922 Loss 0.4324
* VALID - Accuracy 83.970 Loss 0.4624
Epoch:8
LR: 0.001
* TRAIN - Accuracy 85.910 Loss 0.4011
* VALID - Accuracy 82.500 Loss 0.5210
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.692 Loss 0.3811
* VALID - Accuracy 82.400 Loss 0.5364
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.614 Loss 0.3569
* VALID - Accuracy 82.840 Loss 0.5346
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.188 Loss 0.3380
* VALID - Accuracy 84.680 Loss 0.4705
* VALID - Accuracy 84.680 Loss 0.4705
OrderedDict([('All', {'All': 84.68})])
Task All average acc: 84.68
===Summary of experiment repeats: 8 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11 85.94 82.6  84.68  0.    0.  ]
mean: 67.88 std: 33.975828172393385
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 49.310 Loss 1.3821
* VALID - Accuracy 53.070 Loss 1.4688
Epoch:1
LR: 0.001
* TRAIN - Accuracy 68.744 Loss 0.8761
* VALID - Accuracy 69.790 Loss 0.9106
Epoch:2
LR: 0.001
* TRAIN - Accuracy 74.868 Loss 0.7141
* VALID - Accuracy 78.030 Loss 0.6293
Epoch:3
LR: 0.001
* TRAIN - Accuracy 78.522 Loss 0.6130
* VALID - Accuracy 75.850 Loss 0.7229
Epoch:4
LR: 0.001
* TRAIN - Accuracy 80.944 Loss 0.5462
* VALID - Accuracy 77.900 Loss 0.6686
Epoch:5
LR: 0.001
* TRAIN - Accuracy 82.902 Loss 0.4946
* VALID - Accuracy 79.920 Loss 0.5985
Epoch:6
LR: 0.001
* TRAIN - Accuracy 84.150 Loss 0.4588
* VALID - Accuracy 81.100 Loss 0.5719
Epoch:7
LR: 0.001
* TRAIN - Accuracy 85.498 Loss 0.4226
* VALID - Accuracy 81.410 Loss 0.5516
Epoch:8
LR: 0.001
* TRAIN - Accuracy 86.434 Loss 0.3915
* VALID - Accuracy 83.110 Loss 0.5070
Epoch:9
LR: 0.001
* TRAIN - Accuracy 87.294 Loss 0.3700
* VALID - Accuracy 84.780 Loss 0.4468
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.764 Loss 0.3529
* VALID - Accuracy 85.870 Loss 0.4165
Epoch:11
LR: 0.001
* TRAIN - Accuracy 88.438 Loss 0.3339
* VALID - Accuracy 83.900 Loss 0.4877
* VALID - Accuracy 83.900 Loss 0.4877
OrderedDict([('All', {'All': 83.9})])
Task All average acc: 83.9
===Summary of experiment repeats: 9 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11 85.94 82.6  84.68 83.9   0.  ]
mean: 76.27 std: 25.47271873985971
Files already downloaded and verified
Files already downloaded and verified
split_boundaries: [0, 2, 4, 6, 8, 10]
{'1': [0, 1], '2': [2, 3], '3': [4, 5], '4': [6, 7], '5': [8, 9]}
CNN(
  (c1): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c2): Sequential(
    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (c3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fc1): Sequential(
    (0): Linear(in_features=2048, out_features=256, bias=True)
    (1): ReLU()
  )
  (last): ModuleDict(
    (All): Linear(in_features=256, out_features=10, bias=True)
  )
)
#parameter of model: 962858
Task order: ['1', '2', '3', '4', '5']
Epoch:0
LR: 0.001
* TRAIN - Accuracy 47.684 Loss 1.4224
* VALID - Accuracy 57.700 Loss 1.1863
Epoch:1
LR: 0.001
* TRAIN - Accuracy 66.296 Loss 0.9327
* VALID - Accuracy 65.630 Loss 1.0270
Epoch:2
LR: 0.001
* TRAIN - Accuracy 73.718 Loss 0.7426
* VALID - Accuracy 72.760 Loss 0.8074
Epoch:3
LR: 0.001
* TRAIN - Accuracy 77.616 Loss 0.6417
* VALID - Accuracy 75.860 Loss 0.7162
Epoch:4
LR: 0.001
* TRAIN - Accuracy 79.996 Loss 0.5748
* VALID - Accuracy 74.690 Loss 0.7451
Epoch:5
LR: 0.001
* TRAIN - Accuracy 81.748 Loss 0.5227
* VALID - Accuracy 77.590 Loss 0.6758
Epoch:6
LR: 0.001
* TRAIN - Accuracy 83.324 Loss 0.4785
* VALID - Accuracy 80.560 Loss 0.5912
Epoch:7
LR: 0.001
* TRAIN - Accuracy 84.486 Loss 0.4479
* VALID - Accuracy 84.030 Loss 0.4780
Epoch:8
LR: 0.001
* TRAIN - Accuracy 85.568 Loss 0.4162
* VALID - Accuracy 85.020 Loss 0.4513
Epoch:9
LR: 0.001
* TRAIN - Accuracy 86.466 Loss 0.3941
* VALID - Accuracy 84.740 Loss 0.4622
Epoch:10
LR: 0.001
* TRAIN - Accuracy 87.148 Loss 0.3709
* VALID - Accuracy 83.170 Loss 0.5117
Epoch:11
LR: 0.001
* TRAIN - Accuracy 87.902 Loss 0.3488
* VALID - Accuracy 85.660 Loss 0.4245
* VALID - Accuracy 85.660 Loss 0.4245
OrderedDict([('All', {'All': 85.66})])
Task All average acc: 85.66
===Summary of experiment repeats: 10 / 10 ===
The regularization coefficient: 0.0
The last avg acc of all repeats: [81.86 86.66 86.54 86.41 84.11 85.94 82.6  84.68 83.9  85.66]
mean: 84.83599999999998 std: 1.6090257922109268
reg_coef: 0.0 mean: 84.83599999999998 std: 1.6090257922109268
